{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c69b4b8-3167-4cd0-8f61-a56139299c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER'S PROJECT: Toxic messages handling project\n",
      "======================================================================\n",
      "üì• Loading data from Excel file...\n",
      "‚úÖ Successfully loaded from Excel\n",
      "Original dataset size: (469967, 9)\n",
      "\n",
      "üìä Basic dataset information:\n",
      "Columns: ['Column1', 'raw_text_id', 'dataset_id', 'source_platform', 'nickname', 'is_verified', 'text_raw', 'is_toxic', 'toxicity_type']\n",
      "Data types:\n",
      "Column1              int64\n",
      "raw_text_id          int64\n",
      "dataset_id           int64\n",
      "source_platform     object\n",
      "nickname            object\n",
      "is_verified          int64\n",
      "text_raw            object\n",
      "is_toxic           float64\n",
      "toxicity_type       object\n",
      "dtype: object\n",
      "\n",
      "üëÄ First 5 rows of data:\n",
      "   Column1  raw_text_id  dataset_id source_platform nickname  is_verified  \\\n",
      "0        0            0           0     2ch, pikabu      NaN            1   \n",
      "1        1            1           0     2ch, pikabu      NaN            1   \n",
      "2        2            2           0     2ch, pikabu      NaN            1   \n",
      "3        3            3           0     2ch, pikabu      NaN            1   \n",
      "4        4            4           0     2ch, pikabu      NaN            1   \n",
      "\n",
      "                                            text_raw  is_toxic toxicity_type  \n",
      "0               –í–µ—Ä–±–ª—é–¥–æ–≤-—Ç–æ –∑–∞ —á—Ç–æ? –î–µ–±–∏–ª—ã, –±–ª...\\n       1.0           NaN  \n",
      "1  –•–æ—Ö–ª—ã, —ç—Ç–æ –æ—Ç–¥—É—à–∏–Ω–∞ –∑–∞—Ç—é–∫–∞–Ω–æ–≥–æ —Ä–æ—Å—Å–∏—è–Ω–∏–Ω–∞, –º–æ–ª...       1.0           NaN  \n",
      "2                          –°–æ–±–∞–∫–µ - —Å–æ–±–∞—á—å—è —Å–º–µ—Ä—Ç—å\\n       1.0           NaN  \n",
      "3  –°—Ç—Ä–∞–Ω–∏—Ü—É –æ–±–Ω–æ–≤–∏, –¥–µ–±–∏–ª. –≠—Ç–æ —Ç–æ–∂–µ –Ω–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏...       1.0           NaN  \n",
      "4  —Ç–µ–±—è –Ω–µ —É–±–µ–¥–∏–ª 6-—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π –ø–¥—Ñ –≤ —Ç–æ–º, —á—Ç–æ –°–∫—Ä...       1.0           NaN  \n",
      "\n",
      "‚úÖ Available expected columns: ['text_raw', 'is_toxic', 'toxicity_type', 'source_platform']\n",
      "\n",
      "üìù Sample text_raw values:\n",
      "  1: –í–µ—Ä–±–ª—é–¥–æ–≤-—Ç–æ –∑–∞ —á—Ç–æ? –î–µ–±–∏–ª—ã, –±–ª...\n",
      "...\n",
      "  2: –•–æ—Ö–ª—ã, —ç—Ç–æ –æ—Ç–¥—É—à–∏–Ω–∞ –∑–∞—Ç—é–∫–∞–Ω–æ–≥–æ —Ä–æ—Å—Å–∏—è–Ω–∏–Ω–∞, –º–æ–ª, –≤–æ–Ω, –∞ —É —Ö–æ—Ö–ª–æ–≤ –µ—â–µ —Ö—É–∂–µ. –ï—Å–ª–∏ –±—ã —Ö–æ—Ö–ª–æ–≤ –Ω–µ –±—ã–ª–æ, –∫–∏...\n",
      "  3: –°–æ–±–∞–∫–µ - —Å–æ–±–∞—á—å—è —Å–º–µ—Ä—Ç—å\n",
      "...\n",
      "\n",
      "üî¢ is_toxic unique values: [ 1.  0. nan]\n",
      "üè∑Ô∏è toxicity_type unique values: [nan 'INSULT' 'NORMAL' 'OBSCENITY' 'THREAT' 'INAPPROPRIATE' 'SENSITIVE']\n",
      "üåê source_platform unique values: ['2ch, pikabu' 'ok.ru' '2ch.hk, Pikabu.ru, otveti.mail.ru' '2ch, vk'\n",
      " '–º–µ–¥—É–∑–∞']\n",
      "\n",
      "üîç Missing values:\n",
      "  nickname: 468003 missing (99.6%)\n",
      "  is_toxic: 124586 missing (26.5%)\n",
      "  toxicity_type: 97098 missing (20.7%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download stopwords for Russian language / –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"MASTER'S PROJECT: Toxic messages handling project\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING - EXCEL FILE / –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üì• Loading data from Excel file...\")\n",
    "file_path = \"MergedDS.xlsx\"  \n",
    "\n",
    "try:\n",
    "    # Try to read Excel file / –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel —Ñ–∞–π–ª\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"‚úÖ Successfully loaded from Excel\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Excel loading failed: {e}\")\n",
    "    print(\"üîÑ Trying CSV with different separators...\")\n",
    "    \n",
    "    # Fallback to CSV with different separators / –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏\n",
    "    separators = ['\\t', ',', ';', '|']\n",
    "    \n",
    "    for sep in separators:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path.replace('.xlsx', '.csv'), sep=sep, encoding='utf-8')\n",
    "            print(f\"‚úÖ Loaded with separator: '{sep}'\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        print(\"‚ùå All loading methods failed\")\n",
    "        exit()\n",
    "\n",
    "print(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "# Basic information / –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "print(\"\\nüìä Basic dataset information:\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Display first few rows to check data / –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\nüëÄ First 5 rows of data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check if we have the expected columns / –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "expected_columns = ['text_raw', 'is_toxic', 'toxicity_type', 'source_platform']\n",
    "available_columns = [col for col in expected_columns if col in df.columns]\n",
    "print(f\"\\n‚úÖ Available expected columns: {available_columns}\")\n",
    "\n",
    "# Show sample data from key columns / –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "if 'text_raw' in df.columns:\n",
    "    print(f\"\\nüìù Sample text_raw values:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        text_sample = str(df['text_raw'].iloc[i])[:100]\n",
    "        print(f\"  {i+1}: {text_sample}...\")\n",
    "\n",
    "if 'is_toxic' in df.columns:\n",
    "    print(f\"\\nüî¢ is_toxic unique values: {df['is_toxic'].unique()}\")\n",
    "\n",
    "if 'toxicity_type' in df.columns:\n",
    "    print(f\"üè∑Ô∏è toxicity_type unique values: {df['toxicity_type'].unique()}\")\n",
    "\n",
    "if 'source_platform' in df.columns:\n",
    "    print(f\"üåê source_platform unique values: {df['source_platform'].unique()[:5]}\")\n",
    "\n",
    "# Missing values analysis / –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "print(f\"\\nüîç Missing values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "for col, count in missing_data.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} missing ({count/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d605dee-466c-4352-a2a6-41c47fe250b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Data cleaning and type conversion...\n",
      "Cleaned column names: ['Column1', 'raw_text_id', 'dataset_id', 'source_platform', 'nickname', 'is_verified', 'text_raw', 'is_toxic', 'toxicity_type']\n",
      "is_toxic before conversion: [ 1.  0. nan]\n",
      "is_toxic after conversion: [ 1.  0. nan]\n",
      "Cleaned text_raw column\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. DATA CLEANING AND TYPE CONVERSION / –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• –ò –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –¢–ò–ü–û–í\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîß Data cleaning and type conversion...\")\n",
    "\n",
    "# Clean column names / –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫\n",
    "df.columns = df.columns.str.strip()\n",
    "print(f\"Cleaned column names: {df.columns.tolist()}\")\n",
    "\n",
    "# Convert is_toxic to numeric / –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º is_toxic –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç\n",
    "if 'is_toxic' in df.columns:\n",
    "    print(f\"is_toxic before conversion: {df['is_toxic'].unique()}\")\n",
    "    df['is_toxic'] = pd.to_numeric(df['is_toxic'], errors='coerce')\n",
    "    print(f\"is_toxic after conversion: {df['is_toxic'].unique()}\")\n",
    "\n",
    "# Clean text_raw column / –û—á–∏—Å—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ text_raw\n",
    "if 'text_raw' in df.columns:\n",
    "    # Remove extra quotes and whitespace / –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã\n",
    "    df['text_raw'] = df['text_raw'].astype(str).str.strip().str.strip('\"')\n",
    "    print(\"Cleaned text_raw column\")\n",
    "\n",
    "# Convert other numeric columns / –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "numeric_columns = ['raw_text_id', 'dataset_id', 'is_verified']\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f08f2eb-b13b-4e56-8169-fe50187a17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è Processing missing labels...\n",
      "is_toxic unique values: [ 1.  0. nan]\n",
      "toxicity_type unique values: [nan 'INSULT' 'NORMAL' 'OBSCENITY' 'THREAT' 'INAPPROPRIATE' 'SENSITIVE']\n",
      "Filled is_toxic=1 for 124586 rows (toxicity_type indicates toxicity)\n",
      "Filled toxicity_type='NORMAL' for 65333 rows\n",
      "Converted is_toxic to integer type\n",
      "\n",
      "üìä After filling missing labels:\n",
      "is_toxic missing: 0 (0.0%)\n",
      "toxicity_type missing: 31765 (6.8%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. MISSING LABELS HANDLING / –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ü–£–©–ï–ù–ù–´–• –ú–ï–¢–û–ö\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Processing missing labels...\")\n",
    "\n",
    "if 'is_toxic' in df.columns and 'toxicity_type' in df.columns:\n",
    "    # Check current state of labels / –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–µ—Ç–æ–∫\n",
    "    print(f\"is_toxic unique values: {df['is_toxic'].unique()}\")\n",
    "    print(f\"toxicity_type unique values: {df['toxicity_type'].unique()}\")\n",
    "\n",
    "    # Logic for filling missing values: / –õ–æ–≥–∏–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤:\n",
    "    # 1. If is_toxic is empty but toxicity_type is filled (not empty and not NORMAL) ‚Üí is_toxic = 1\n",
    "    # 1. –ï—Å–ª–∏ is_toxic –ø—É—Å—Ç–æ–µ, –Ω–æ toxicity_type –∑–∞–ø–æ–ª–Ω–µ–Ω–æ (–Ω–µ –ø—É—Å—Ç–æ–µ –∏ –Ω–µ NORMAL) ‚Üí is_toxic = 1\n",
    "    mask_toxic_missing = df['is_toxic'].isna()\n",
    "    mask_tox_type_filled = (~df['toxicity_type'].isna()) & (df['toxicity_type'] != '') & (df['toxicity_type'] != 'NORMAL')\n",
    "    \n",
    "    fill_toxic_count = (mask_toxic_missing & mask_tox_type_filled).sum()\n",
    "    df.loc[mask_toxic_missing & mask_tox_type_filled, 'is_toxic'] = 1\n",
    "    print(f\"Filled is_toxic=1 for {fill_toxic_count} rows (toxicity_type indicates toxicity)\")\n",
    "\n",
    "    # 2. If is_toxic = 0, and toxicity_type is empty ‚Üí toxicity_type = 'NORMAL'\n",
    "    # 2. –ï—Å–ª–∏ is_toxic = 0, –∞ toxicity_type –ø—É—Å—Ç–æ–µ ‚Üí toxicity_type = 'NORMAL'\n",
    "    mask_toxic_zero = df['is_toxic'] == 0\n",
    "    mask_tox_type_empty = (df['toxicity_type'].isna()) | (df['toxicity_type'] == '')\n",
    "    \n",
    "    fill_normal_count = (mask_toxic_zero & mask_tox_type_empty).sum()\n",
    "    df.loc[mask_toxic_zero & mask_tox_type_empty, 'toxicity_type'] = 'NORMAL'\n",
    "    print(f\"Filled toxicity_type='NORMAL' for {fill_normal_count} rows\")\n",
    "\n",
    "    # Convert is_toxic to integer / –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º is_toxic –≤ integer \n",
    "    if df['is_toxic'].dtype == 'float64':\n",
    "        df['is_toxic'] = df['is_toxic'].astype('Int64')\n",
    "        print(\"Converted is_toxic to integer type\")\n",
    "\n",
    "    # Check results after filling / –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "    print(f\"\\nüìä After filling missing labels:\")\n",
    "    print(f\"is_toxic missing: {df['is_toxic'].isna().sum()} ({df['is_toxic'].isna().sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"toxicity_type missing: {df['toxicity_type'].isna().sum()} ({df['toxicity_type'].isna().sum()/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f62290-4696-40b3-8331-367091eed8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Starting text preprocessing...\n",
      "Removed empty texts: 0\n",
      "Cleaning texts...\n",
      "After removing short texts: 469901 rows (removed 66)\n",
      "Removed duplicates: 19691\n",
      "Removing stopwords...\n",
      "Final dataset size after preprocessing: 450210 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. TEXT PREPROCESSING / –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–ê\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüßπ Starting text preprocessing...\")\n",
    "\n",
    "if 'text_raw' in df.columns:\n",
    "    # Save initial number of rows / –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫\n",
    "    initial_rows = len(df)\n",
    "\n",
    "    # 1. Remove completely empty rows / –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫\n",
    "    df = df.dropna(subset=['text_raw'])\n",
    "    df = df[df['text_raw'] != '']\n",
    "    df = df[df['text_raw'] != 'nan']\n",
    "    print(f\"Removed empty texts: {initial_rows - len(df)}\")\n",
    "\n",
    "    # Text cleaning function / –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "    def clean_text_advanced(text):\n",
    "        if pd.isna(text) or text == '' or text == 'nan':\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Convert to lowercase / –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        text = text.lower()\n",
    "        \n",
    "        # HTML entities decoding / –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        import html\n",
    "        try:\n",
    "            text = html.unescape(text)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Remove HTML tags / –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs / –£–¥–∞–ª–µ–Ω–∏–µ URL\n",
    "        text = re.sub(r'http\\S+', ' URL ', text)\n",
    "        \n",
    "        # Remove emails / –£–¥–∞–ª–µ–Ω–∏–µ email\n",
    "        text = re.sub(r'\\S+@\\S+', ' EMAIL ', text)\n",
    "        \n",
    "        # Replace numbers / –ó–∞–º–µ–Ω–∞ —á–∏—Å–µ–ª\n",
    "        text = re.sub(r'\\d+', ' NUMBER ', text)\n",
    "        \n",
    "        # Process emoticons / –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–æ—Ç–∏–∫–æ–Ω–æ–≤\n",
    "        text = re.sub(r'[:;=]-?[\\)\\(DP]', ' EMOTICON ', text)\n",
    "        \n",
    "        # Process emojis / –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–æ–¥–∑–∏\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons / —ç–º–æ—Ü–∏–∏\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs / —Å–∏–º–≤–æ–ª—ã –∏ –ø–∏–∫—Ç–æ–≥—Ä–∞–º–º—ã\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols / —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –∫–∞—Ä—Ç—ã\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS) / —Ñ–ª–∞–≥–∏\n",
    "            u\"\\U00002702-\\U000027B0\"  # other symbols / –¥—Ä—É–≥–∏–µ —Å–∏–º–≤–æ–ª—ã\n",
    "            u\"\\U000024C2-\\U0001F251\" \n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r' EMOJI ', text)\n",
    "        \n",
    "        # Process repeated punctuation (2+ characters) / –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–π—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (2+ —Å–∏–º–≤–æ–ª–∞)\n",
    "        text = re.sub(r'[!?]{2,}', ' REPEAT_PUNCT ', text)  # !! ??? !!!\n",
    "        text = re.sub(r'\\.{2,}', ' REPEAT_PUNCT ', text)    # ... .... ‚Üí REPEAT_PUNCT\n",
    "        text = re.sub(r'[!?]\\.+', ' REPEAT_PUNCT ', text)   # !.. ?... ‚Üí REPEAT_PUNCT\n",
    "        text = re.sub(r'[!?][!?]+', ' REPEAT_PUNCT ', text) # !? !!? ?! ‚Üí REPEAT_PUNCT\n",
    "        \n",
    "        # Remove special characters but keep Russian letters / –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤\n",
    "        text = re.sub(r'[^\\w\\s–∞-—è—ë]', ' ', text)\n",
    "        \n",
    "        # Remove extra spaces / –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    # Apply cleaning / –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É\n",
    "    print(\"Cleaning texts...\")\n",
    "    df['clean_text'] = df['text_raw'].apply(clean_text_advanced)\n",
    "\n",
    "    # Remove very short texts (less than 3 characters) / –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    initial_count = len(df)\n",
    "    df = df[df['clean_text'].str.len() >= 3]\n",
    "    print(f\"After removing short texts: {len(df)} rows (removed {initial_count - len(df)})\")\n",
    "\n",
    "    # Remove text duplicates after cleaning / –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['clean_text'])\n",
    "    print(f\"Removed duplicates: {initial_count - len(df)}\")\n",
    "\n",
    "    # Remove stopwords / –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    def remove_stopwords(text):\n",
    "        if not isinstance(text, str) or text == '':\n",
    "            return \"\"\n",
    "        \n",
    "        russian_stopwords = set(stopwords.words('russian'))\n",
    "        custom_stopwords = {\n",
    "            '—ç—Ç–æ', '–≤–æ—Ç', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ', '–Ω–µ', '–Ω–∞', '–Ω–æ', '–∑–∞', \n",
    "            '—Ç–æ', '—Å', '–ª–∏', '–∞', '–≤–æ', '–æ—Ç', '—Å–æ', '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', '–±—ã', '—á—Ç–æ',\n",
    "            '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–≤–∞—Å', '–≤–∞–º', '—Ç–µ–±–µ', '—Ç—ã', '–º–Ω–µ',\n",
    "            '–º–µ–Ω—è', '–µ–º—É', '–µ–π', '–∏–º', '–Ω–∏–º–∏', \n",
    "            'NUMBER', 'URL', 'EMAIL', 'EMOTICON', 'EMOJI', 'REPEAT_PUNCT'\n",
    "        }\n",
    "        \n",
    "        all_stopwords = russian_stopwords.union(custom_stopwords)\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in all_stopwords and len(word) > 2]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    print(\"Removing stopwords...\")\n",
    "    df['clean_text_no_stopwords'] = df['clean_text'].apply(remove_stopwords)\n",
    "\n",
    "    print(f\"Final dataset size after preprocessing: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4279b4d6-1911-46b6-b17a-27371b0a946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASIC DATASET STATISTICS\n",
      "============================================================\n",
      "Total records: 450,210\n",
      "Toxic records: 195,151 (43.3%)\n",
      "Non-toxic records: 255,059 (56.7%)\n",
      "\n",
      "Text length (in words):\n",
      "Average: 19.1\n",
      "After stopwords removal: 11.0\n",
      "Maximum: 3213\n",
      "Minimum: 1\n",
      "\n",
      "üåê Source platform analysis:\n",
      "  ok.ru: 244,771 records (54.4%)\n",
      "  2ch.hk, Pikabu.ru, otveti.mail.ru: 124,435 records (27.6%)\n",
      "  2ch, vk: 60,673 records (13.5%)\n",
      "  2ch, pikabu: 14,152 records (3.1%)\n",
      "  YouTube: 1,937 records (0.4%)\n",
      "  Social Media, TV-Scripts (South Park): 1,450 records (0.3%)\n",
      "  –ª–µ–Ω—Ç–∞—á: 981 records (0.2%)\n",
      "  –º–µ–¥—É–∑–∞: 932 records (0.2%)\n",
      "  –¥–æ–∂–¥—å: 879 records (0.2%)\n",
      "\n",
      "‚úÖ Verification analysis:\n",
      "  1: 383,358 records (85.2%)\n",
      "  0: 66,852 records (14.8%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. BASIC DATASET STATISTICS / –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASIC DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "\n",
    "# Analyze is_toxic if available / –ê–Ω–∞–ª–∏–∑ is_toxic –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "if 'is_toxic' in df.columns:\n",
    "    toxic_count = df['is_toxic'].sum()\n",
    "    normal_count = len(df) - toxic_count\n",
    "    print(f\"Toxic records: {toxic_count:,} ({df['is_toxic'].mean():.1%})\")\n",
    "    print(f\"Non-toxic records: {normal_count:,} ({(1 - df['is_toxic'].mean()):.1%})\")\n",
    "\n",
    "# Text length if available / –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "if 'clean_text' in df.columns:\n",
    "    df['text_length'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "    df['text_length_no_stopwords'] = df['clean_text_no_stopwords'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    print(f\"\\nText length (in words):\")\n",
    "    print(f\"Average: {df['text_length'].mean():.1f}\")\n",
    "    print(f\"After stopwords removal: {df['text_length_no_stopwords'].mean():.1f}\")\n",
    "    print(f\"Maximum: {df['text_length'].max()}\")\n",
    "    print(f\"Minimum: {df['text_length'].min()}\")\n",
    "\n",
    "# Analyze source_platform if available / –ê–Ω–∞–ª–∏–∑ source_platform –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "if 'source_platform' in df.columns:\n",
    "    print(f\"\\nüåê Source platform analysis:\")\n",
    "    platform_stats = df['source_platform'].value_counts()\n",
    "    for platform, count in platform_stats.head(10).items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {platform}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze is_verified if available / –ê–Ω–∞–ª–∏–∑ is_verified –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "if 'is_verified' in df.columns:\n",
    "    print(f\"\\n‚úÖ Verification analysis:\")\n",
    "    verified_stats = df['is_verified'].value_counts()\n",
    "    for verified, count in verified_stats.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {verified}: {count:,} records ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1938c45f-ae86-46ab-98fe-8a6bcb672c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving processed data...\n",
      "‚úÖ Data saved to: processed_toxic_comments_ds.xlsx\n",
      "üìä Records saved: 450,210\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. SAVE PROCESSED DATA / –°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüíæ Saving processed data...\")\n",
    "\n",
    "# Save to Excel\n",
    "output_path = \"processed_toxic_comments_ds.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"‚úÖ Data saved to: {output_path}\")\n",
    "print(f\"üìä Records saved: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3708e-a5d1-4e4f-a648-d82b19cb6a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
