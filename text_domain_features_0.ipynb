{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Features engineering.v0\n",
    "---\n",
    "## Notebook contents ##\n",
    "This notebook contains the example of data **normalization** and **feature composition** in text domain. After its execution, the results are\n",
    "* New columns in merged data.csv: added preprocessed version of the initial data; added created features. \n",
    "* The conclusion of its correlation with the target var and how can we use them \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast \n",
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "# import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "# import great_expectations as gx\n",
    "# import great_expectations.expectations as gxe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Normalize text and encode some patterns\n",
    "The base data to train the classical ML algorithms is a fully cleaned, simplified, shorted texts, splitted into the words and (optional) lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_csv = 'data/raw/df_common.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_common_csv, index_col=0)\n",
    "print('Initial shape', df.shape)\n",
    "df.drop_duplicates(subset='text_raw')\n",
    "print('Shape after drop duplicates', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Process the existing values in the square brackets\n",
    "Raw datasets may contain their own encoding or tokenizing styles,  \n",
    "so analyze and transform it into the unified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calc_num_of_square_br(column: pd.Series): \n",
    "    \"\"\"Process values in square brackets: \"\"\"\n",
    "\n",
    "    pattern = re.compile(r'\\[([^\\[\\]]+)\\]')  \n",
    "    all_in_brackets = column.dropna().apply(lambda x: pattern.findall(x))\n",
    "\n",
    "    flat_list = [word for sublist in all_in_brackets for word in sublist]\n",
    "    counter = Counter(flat_list)\n",
    "    most_common = counter.most_common(20)\n",
    "\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_num_of_square_br(df['text_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'Ссылка' на [LINK]: \n",
    "patterns_to_replace = [\n",
    "    r'\\[ссылка заблокирована по решению администрации проекта\\]',\n",
    "    r'\\[Ссылка\\]',\n",
    "    r'\\[ссылка\\]'\n",
    "]\n",
    "\n",
    "replace_count = 0\n",
    "for pattern in patterns_to_replace:\n",
    "    count = df['text_raw'].str.count(pattern).sum()\n",
    "    replace_count += count\n",
    "    df['text_raw'] = df['text_raw'].str.replace(pattern, '[URL]', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines how to map the substrings and replace it in the initial dataset\n",
    "# We already have [LINK], [NUMBER]\n",
    "\n",
    "# REPEAT_PUNCT, number, \n",
    "mapping_dict = {\n",
    "    \"url\": \"[URL]\",\n",
    "    \"num\": \"[NUM]\", # \n",
    "    \"mention\": \"[MNT]\",\n",
    "    \"hashtag\": \"[HSG]\",\n",
    "    \"email\": \"[EML]\",\n",
    "    \"repeat_punct\": \"[RPP]\",\n",
    "    # \"emoticon\": \"[EMOTICON]\",\n",
    "    # \"emoji\": \"[EMOJI]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Process non-informatives: URLs, hashtags, numbers etc.\n",
    "In this part of the code, we are cleaning non-informative substrings that are making texts noisy.   \n",
    "These substrings can be deleted without no information loss   \n",
    "(but it will be checked by correlations analysis in p.2 too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def map_noninformatives(text, mapping_dict: dict):\n",
    "    \"\"\"Mapping extra str that does not contain useful or informative substrings: \n",
    "    Cleaning of URLs, mentiones, hashtags, numbers, emails, HTML symbols, do strip\n",
    "    Uses the dict of mapping\n",
    "    \n",
    "    Regexps are written by GPT\"\"\"\n",
    "\n",
    "    def clean_html(text):\n",
    "        \"\"\"Cleaning of html entities. Regexps are written by GPT\"\"\"\n",
    "        # Replace <br> and its variations with a space\n",
    "        text = re.sub(r'<br\\s*/?>', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove all other HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Decode HTML entities (e.g. &quot;, &amp;, &#39;)\n",
    "        text = html.unescape(text)\n",
    "        \n",
    "        # Remove extra spaces and trim\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Replace urls: \n",
    "    text = re.sub(r\"http\\S+|www\\S+\", mapping_dict.get('url'), text)\n",
    "    \n",
    "    # Replace mentions: \n",
    "    text = re.sub(r\"[@]\\w+\", mapping_dict.get('mention'), text)\n",
    "    text = re.sub(r'id\\d+\\|[^\\s]+', mapping_dict.get('mention'), text) # based on info above\n",
    "\n",
    "    # replace hashtags: \n",
    "    text = re.sub(r\"[#]\\w+\", mapping_dict.get('hashtag'), text) \n",
    "\n",
    "    # Replace numbers (integer or decimal):\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\b|NUMBER|number', mapping_dict.get('num'), text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace emails: \n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', mapping_dict.get('email'), text)\n",
    "\n",
    "    # Delete br and quot, other HTML tags: \n",
    "    # text = re.sub(r'<br\\s*/?>', ' ', text, flags=re.IGNORECASE)\n",
    "    # text = re.sub(r'&quot;', '', text, flags=re.IGNORECASE)\n",
    "    text = clean_html(text)\n",
    "    \n",
    "    # collapse multiple spaces and strip: \n",
    "    # text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "tqdm.pandas(desc=\"Mapping non-informative symbols\")\n",
    "\n",
    "df['text_encoded_noninfrm'] = df['text_raw'].progress_apply(lambda r: map_noninformatives(r, mapping_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the result, we replaced some frequent mentiones, specific patterns\n",
    "calc_num_of_square_br(df['text_encoded_noninfrm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Process emojis, emoticons\n",
    "Here we are encoding some special substrings that may be informative and can be used as features, like emoji or emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_dict_to_json(data_dict, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Encoding dict to replace its values in the text: \n",
    "encoding_emoji = dict()\n",
    "\n",
    "# Encoding dict of emoticons to replace its values in the text: \n",
    "encoding_emoticon = dict()\n",
    "\n",
    "def map_emoji_emoticons(text: str, \n",
    "                        mapping_dicts: tuple[dict, dict], \n",
    "                        emoji_regrow=r'',\n",
    "                        token_emoji='EMJ',\n",
    "                        token_emoticon='EMT'):\n",
    "    encoding_emoji, encoding_emoticon = mapping_dicts\n",
    "\n",
    "    def encode_emoji(encoding_text: str):\n",
    "        emoji_list = emoji.emoji_list(encoding_text)\n",
    "        for e in sorted(set([item['emoji'] for item in emoji_list])):\n",
    "            if e not in encoding_emoji: \n",
    "                token = f\"[{token_emoji}_{len(encoding_emoji)}]\" \n",
    "                encoding_emoji[e] = token\n",
    "            encoding_text = encoding_text.replace(e, encoding_emoji[e])\n",
    "        return encoding_text\n",
    "\n",
    "    def encode_emoticon(encoding_text: str, regrow=emoji_regrow):\n",
    "        emoticon_pattern = re.compile(regrow)\n",
    "        emoticons_found = set(re.findall(emoticon_pattern, encoding_text))\n",
    "        for emo in emoticons_found:\n",
    "            if emo not in encoding_emoticon:\n",
    "                token = f\"[{token_emoticon}_{len(encoding_emoticon)}]\"\n",
    "                encoding_emoticon[emo] = token\n",
    "            encoding_text = encoding_text.replace(emo, encoding_emoticon[emo])\n",
    "        return encoding_text\n",
    "\n",
    "    text = encode_emoji(text)\n",
    "    text = encode_emoticon(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regexp is writte by GPT to process different emoticons: \n",
    "reg_row = r\"\"\"(?x)  # verbose mode\n",
    "(?:                             # group of emoticons\n",
    "    (?::|;|=|8)                 # leading eyes for Western style\n",
    "    (?:-)?                      # optional nose\n",
    "    (?:\\)|\\(|D|P|p|O|o|3|/|\\\\|\\||\\*|\\$|@)   # mouth / expression\n",
    "        |\n",
    "    (?:\\^\\^|_\\^|^_\\^|^‿^|˘‿˘)   # simple happy eyes/mouth combos such as ^_^ _^ ^_^ ˘‿˘\n",
    "        |\n",
    "    (?:T_T|;_;|;\\-\\;|>_<|>\\.<|>_>|<_<)  # sad/embarrassed/frustrated\n",
    "        |\n",
    "    (?:<3|♥|♡)                # heart symbols\n",
    "        |\n",
    "    (?:¯\\\\_\\(ツ\\)_/¯|¯\\\\_\\(ಠ_ಠ\\)_/¯)  # shrug / disapproval special ones\n",
    "        |\n",
    "    (?:uwu|OwO|UwU|owo)        # internet‑emoticon style\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "tqdm.pandas(desc=\"Mapping emojis, emoticons\")\n",
    "\n",
    "# Compose new colunmn\n",
    "df['text_encoded_emoj_emotic'] = df['text_encoded_noninfrm'].progress_apply(\n",
    "    lambda r: map_emoji_emoticons(r, (encoding_emoji, encoding_emoticon), reg_row)\n",
    ")\n",
    "print('Found unique emoji:', len(encoding_emoji))\n",
    "print('Found unique emoticons:', len(encoding_emoticon))\n",
    "\n",
    "save_dict_to_json(encoding_emoji, 'encoding_emoji.json')\n",
    "save_dict_to_json(encoding_emoticon, 'encoding_emoticon.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Process punctuation\n",
    "Punctuation processing includes the next steps \n",
    "1. Define the unique variations of the punctuation repeatings, ... !!! for example, etc. It also will process the residial (or rare) emoticons. \n",
    "2. Encode them and add to the mapping dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation encoding part \n",
    "encoding_rep_punct = dict()\n",
    "encoding_sep_punct = dict()\n",
    "\n",
    "\n",
    "def map_punctuation(\n",
    "        text: str, \n",
    "        mapping_rep_dict: dict, \n",
    "        mapping_sep_dict: dict, \n",
    "        rep_regexp=r\"([!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]{2,})\",\n",
    "        nonrep_regexp=r\"[!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\",\n",
    "        token_sep_punct='SPP', # separate punctuatuin \n",
    "        token_seq_punct='RPP' # repeating punctuation\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Encode both repeating and single punctuation marks with mapping dictionaries.\n",
    "    Sequences inside any [ ... ] (even unclosed) are ignored.\n",
    "    Already existing [TOKEN]-like patterns are ignored too.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def is_inside_token(pos: int) -> bool:\n",
    "        \"\"\"Check if position is inside [TOKEN]-like region.\"\"\"\n",
    "        return any(start <= pos < end for start, end in token_spans)\n",
    "    \n",
    "    def is_protected(pos: int) -> bool:\n",
    "        \"\"\"Return True if position is inside any protected region (token or [ ... ]).\"\"\"\n",
    "        \n",
    "        return is_inside_token(pos) or any(start <= pos < end for start, end in bracket_spans)\n",
    "\n",
    "    def repl_repeat(match):\n",
    "        \"\"\"Handle repeating punctuation\"\"\"\n",
    "\n",
    "        seq = match.group(1)\n",
    "        start = match.start()\n",
    "\n",
    "        if is_protected(start):\n",
    "            return seq\n",
    "\n",
    "        # Unique punctuation marks, preserving order\n",
    "        unique_chars = ''.join(sorted(set(seq), key=seq.index))\n",
    "        normalized = unique_chars[0] if len(unique_chars) == 1 else unique_chars\n",
    "\n",
    "        if normalized not in mapping_rep_dict:\n",
    "            token = f\"[{token_seq_punct}_{len(mapping_rep_dict)}]\"\n",
    "            mapping_rep_dict[normalized] = token\n",
    "        else:\n",
    "            token = mapping_rep_dict[normalized]\n",
    "\n",
    "        return token\n",
    "\n",
    "    def repl_single(match):\n",
    "        \"\"\"Handle separate punctuation\"\"\"\n",
    "\n",
    "        ch = match.group(0)\n",
    "        start = match.start()\n",
    "\n",
    "        if is_protected(start):\n",
    "            return ch\n",
    "\n",
    "        if ch not in mapping_sep_dict:\n",
    "            token = f\"[{token_sep_punct}_{len(mapping_sep_dict)}]\"\n",
    "            mapping_sep_dict[ch] = token\n",
    "        else:\n",
    "            token = mapping_sep_dict[ch]\n",
    "\n",
    "        return token\n",
    "    \n",
    "    # Compile patterns: \n",
    "    rep_pattern = re.compile(rep_regexp)\n",
    "    sep_pattern = re.compile(nonrep_regexp)\n",
    "\n",
    "    # Find existing [TOKEN]-like regions: \n",
    "    token_spans = []\n",
    "    for match in re.finditer(r\"\\[[A-Za-z0-9_]+\\]\", text):\n",
    "        token_spans.append((match.start(), match.end()))\n",
    "\n",
    "    # Detect generic bracket regions (for [ ... ] ), excluding [TOKEN] ones: \n",
    "    bracket_spans = []\n",
    "    open_pos = None\n",
    "    for i, ch in enumerate(text):\n",
    "        if is_inside_token(i):\n",
    "            continue  # Skip positions inside known tokens entirely\n",
    "        if ch == '[':\n",
    "            if open_pos is None:\n",
    "                open_pos = i\n",
    "        elif ch == ']' and open_pos is not None:\n",
    "            bracket_spans.append((open_pos, i + 1))\n",
    "            open_pos = None\n",
    "    if open_pos is not None:  # text ends with unclosed '['\n",
    "        bracket_spans.append((open_pos, len(text)))\n",
    "\n",
    "    # Apply replacements: \n",
    "    new_text = rep_pattern.sub(repl_repeat, text)\n",
    "    new_text = sep_pattern.sub(repl_single, new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Mapping punctuations\")\n",
    "\n",
    "# Encode repeatinfg punctuations, encode non-repeating: \n",
    "df['text_encoded_punct'] = df['text_encoded_emoj_emotic'].progress_apply(\n",
    "    lambda r: map_punctuation(r, encoding_rep_punct, encoding_sep_punct)\n",
    ")\n",
    "\n",
    "print('Found unique signs sequences:', len(encoding_rep_punct))\n",
    "print('Found separated unique signs:', len(encoding_sep_punct))\n",
    "\n",
    "save_dict_to_json(encoding_rep_punct, 'encoding_rep_punct.json')\n",
    "save_dict_to_json(encoding_sep_punct, 'encoding_sep_punct.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install slaviclean==0.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Process profanity words\n",
    "\n",
    "<!-- `WARNING`: \n",
    "map_slang_and_rude preprocessing is used in lemmas handlings and it's working slower than the full BERT-based model inference.  \n",
    "But the main benefit of classical ML algorithms is its better and faster performance, and we don't want to lose it. \n",
    "\n",
    "So there're three variants and steps to resolve this problem: \n",
    "1. Faster inference, worse model quality (by default).\n",
    "2. Slower inference, better model quality (optional) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the prevoiusly composed set of the bad words:  \n",
    "profanity_file = 'bad_words_lemmas.txt'\n",
    "profanities = list()\n",
    "with open(profanity_file, 'r', encoding=\"utf-8\") as f:\n",
    "    profanities_set = [l.strip() for l in f.readlines()]\n",
    "print('Founf N profanities: ', len(profanities_set))\n",
    "\n",
    "# Init base pymorphy analyzer: \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Set the value of mapping profanities dict which are\n",
    "encoding_profanities = dict()\n",
    "\n",
    "def map_profanity(\n",
    "        text: str, \n",
    "        profanities: list,\n",
    "        mapping_dict=None, \n",
    "        token_profanity='PRF' # profanity \n",
    "        # replace_char='[PROFANITY]'\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Map Russian profanity using [what] in text to tokens,\n",
    "    \"\"\"\n",
    "\n",
    "    def repl(match) -> str:\n",
    "        \n",
    "        word = match.group(0)\n",
    "        lower = word.lower()\n",
    "        lemma = morph.parse(lower)[0].normal_form\n",
    "        \n",
    "        if lemma in profanities: \n",
    "            # print('word', word, 'lemma', lemma) # uncomment if you don't need you eyes\n",
    "            if lemma not in mapping_dict:\n",
    "                res = f\"[{token_profanity}_{len(mapping_dict)}]\"\n",
    "                mapping_dict[lemma] = res\n",
    "            else:\n",
    "                res = mapping_dict[lemma]\n",
    "\n",
    "            # print(res)\n",
    "        else: \n",
    "            res = word\n",
    "        \n",
    "        return res\n",
    "\n",
    "    pattern = re.compile(r\"\\b[А-Яа-яЁё']+\\b\")\n",
    "    \n",
    "    return pattern.sub(repl, text)\n",
    "    # return repl(text)\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Mapping profanities\")\n",
    "df['text_encoded_profanity'] = df['text_encoded_punct'].progress_apply(\n",
    "    lambda r: map_profanity(text=r, profanities=profanities_set, mapping_dict=encoding_profanities)\n",
    ")\n",
    "print('Found unique profanities:', len(encoding_profanities))\n",
    "\n",
    "save_dict_to_json(encoding_profanities, 'encoding_profanities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Process 1st, 2nd-person pronouns (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Delete stop words (as the last, optional column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "russian_stopwords = set(get_stop_words('russian'))\n",
    "df['text_del_stop_words'] = df['text_encoded_profanity'].apply(\n",
    "    lambda x: ' '.join(\n",
    "        word for word in x.split() if word.lower() not in russian_stopwords\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Plot wordcloud after processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all tokens: \n",
    "df['text_no_brackets'] = df['text_del_stop_words'].apply(\n",
    "    lambda x: re.sub(r'\\[.*?\\]', '', x)\n",
    ").str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### \n",
    "As the last part, ensure that the most frequent words have sense (code taken from `dataset_eda.ipynb`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text = ' '.join(df['text_no_brackets']).lower()\n",
    "\n",
    "# text_cleaned = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "# Delete spaces: \n",
    "# text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
    "\n",
    "wordcloud_toxic = WordCloud(width=800, height=400, background_color='white',\n",
    "                              colormap='Reds', max_words=100).generate(text)\n",
    "_, axes = plt.subplots(1, 1, figsize=(16, 8))\n",
    "axes.imshow(wordcloud_toxic, interpolation='bilinear')\n",
    "axes.axis('off')\n",
    "axes.set_title('Word Cloud', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dump_features_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "All composed tokens that we can use in feature composition are: \n",
    "* `[URL]` - url\n",
    "* `[NUM]` - number \n",
    "* `[MNT]` - mention\n",
    "* `[HSG]` - hashtag\n",
    "* `[EML]` - email \n",
    "* `[SPP_n]` - n-st type of separate (single) punctuation \n",
    "* `[RPP_n]` - n-st type of repeating punctuation \n",
    "* `[EMJ_n]` - n-st type of emoji\n",
    "* `[EMT_n]`- n-st type of emoticon \n",
    "* `[PRF_n]`- n-st type of  profanity (its lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 2. Define handcrafted features and calculate its correlations with targtet\n",
    "\n",
    "The examples of the handcrafted features are created with using of the previous steps and may have the next variations: \n",
    "\n",
    "\n",
    "**Punctuation:**\n",
    "1. Count of separate punctuations in phrase \n",
    "2. Count of repeating punctuations in phrase \n",
    "3. Is punctuation after spaces in phrase  \n",
    "(for example: '*Some phrase , another part ,  etc*')\n",
    "\n",
    "**Emoji, emoticons:**  \n",
    "1. Is emoji included  \n",
    "2. Is emoticons encluded  \n",
    "\n",
    "\n",
    "**Tonalities in the text structure:**\n",
    "1. Is capslock included (like a screaming tone)   \n",
    "Note: do not include tokens like a  [SOME_TOKEN] to the processing  \n",
    "2. Is all parts in a lowercase  \n",
    "'*this is example. i'm talking a bit tired maybe. the end.*'\n",
    "3. Is punctuation using  \n",
    "'*this is another example talking so fast maybe*'\n",
    "3. Is '*FeNcE IrOnIc StYlE*' included \n",
    "\n",
    "**Tonalities in the lexical content:**\n",
    "1. Count of profanity words included\n",
    "2. Are pronouns included (column **todo**)  \n",
    "Explain: if we have a second-person pronoun and some profanity in the phraze, it's possibly that the message is offensive=>toxic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dump_features_0.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature composing part \n",
    "\n",
    "def is_all_lower(text):\n",
    "    text_no_tokens = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    return int(bool(text_no_tokens) and text_no_tokens.islower())\n",
    "\n",
    "def starts_with_cap(text):\n",
    "    \"\"\"Returns True, if more than a half of sentences in the text start with cap\"\"\"\n",
    "\n",
    "    text_no_tokens = re.sub(r'\\[.*?\\]', '', text).strip()\n",
    "    sentences = re.split(r'[.!?]', text_no_tokens)\n",
    "    starts_with_cap_each = [s.strip()[0].isupper() for s in sentences if s.strip()]\n",
    "    \n",
    "    if not starts_with_cap_each:\n",
    "        return 0  \n",
    "    \n",
    "    # Calculate sums and take a decision: \n",
    "    true_count = sum(starts_with_cap_each)\n",
    "    false_count = len(starts_with_cap_each) - true_count\n",
    "    \n",
    "    return int(true_count > false_count)\n",
    "\n",
    "def has_caps(word_list):\n",
    "    for w in word_list:\n",
    "        if w.isupper() and not re.match(r'\\[.*?\\]', w):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    " \n",
    "df_features = df.copy()\n",
    "df_col_base = df_features['text_encoded_profanity']\n",
    "\n",
    "# 1. Count of separate punctuations (SPP)\n",
    "df_features['count_spp'] = df_col_base.str.count(r'\\[SPP_\\d+\\]')\n",
    "\n",
    "# 2. Count of repeating punctuations (RPP)\n",
    "df_features['count_rpp'] = df_col_base.str.count(r'\\[RPP_\\d+\\]')\n",
    "\n",
    "# 3. Is punctuation after spaces (check SPP after space)\n",
    "df_features['punct_after_space'] = df_col_base.str.contains(r' \\[SPP_\\d+\\]').astype(int)\n",
    "\n",
    "# Emoji, emoticons\n",
    "# 1. Is emoji included: \n",
    "df_features['has_emoji'] = df_col_base.str.contains(r'\\[EMJ_\\d+\\]').astype(int)\n",
    "\n",
    "# 2. Is emoticon included: \n",
    "df_features['has_emoticon'] = df_col_base.str.contains(r'\\[EMT_\\d+\\]').astype(int)\n",
    "\n",
    "# Tonalities in the text structure\n",
    "# 1. Is capslock included (ignoring tokens [SOME_TOKEN]): \n",
    "df_features['has_capslock'] = df_features['text_encoded_profanity'].apply(\n",
    "    lambda x: has_caps(re.findall(r'\\b\\w+\\b', x))\n",
    ").astype(int)\n",
    "\n",
    "# 2. Is all lowercase\n",
    "df_features['is_all_lower'] = df_features['text_encoded_profanity'].apply(is_all_lower)\n",
    "\n",
    "# 3. Is punctuation used: \n",
    "df_features['has_punctuation_spp'] = df_col_base.str.contains(r'\\[SPP_\\d+\\]').astype(int)\n",
    "df_features['has_punctuation_rpp'] = df_col_base.str.contains(r'\\[RPP_\\d+\\]').astype(int)\n",
    "\n",
    "# 4. Is '*FeNcE IrOnIc StYlE*' included: \n",
    "df_features['has_fence_ironic_style'] = df_col_base.str.contains(r'\\*.*?\\*').astype(int)\n",
    "\n",
    "# Tonalities in lexical content\n",
    "# 1. Count of profanity words (PRF): \n",
    "df_features['count_profanity'] = df_col_base.str.count(r'\\[PRF_\\d+\\]')\n",
    "\n",
    "# 2. Are pronouns included (assuming pronouns are inserted как токены, например [PRON_n]): \n",
    "df_features['has_pronouns'] = df_col_base.str.contains(r'\\[PRON_\\d+\\]').astype(int)\n",
    "\n",
    "# Is starting with cap: \n",
    "df_features['starts_with_cap'] = df_features['text_encoded_profanity'].apply(starts_with_cap)\n",
    "\n",
    "# Neutral tokens checking: \n",
    "df_features['has_url'] = df_col_base.str.contains(r'\\[URL\\]').astype(int)\n",
    "df_features['has_number'] = df_col_base.str.contains(r'\\[NUM\\]').astype(int)\n",
    "df_features['has_mention'] = df_col_base.str.contains(r'\\[MNT\\]').astype(int)\n",
    "df_features['has_hashtag'] = df_col_base.str.contains(r'\\[HSG\\]').astype(int)\n",
    "df_features['has_email'] = df_col_base.str.contains(r'\\[EML\\]').astype(int)\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc composed features correlations woth target: \n",
    "composed_features = [\n",
    "    'count_spp',\n",
    "    'count_rpp',\n",
    "    'punct_after_space',\n",
    "    'has_emoji',\n",
    "    'has_emoticon',\n",
    "    'has_capslock',\n",
    "    'is_all_lower',\n",
    "    'has_punctuation_spp',\n",
    "    'has_punctuation_rpp',\n",
    "    'has_fence_ironic_style',\n",
    "    'count_profanity',\n",
    "    'has_pronouns',\n",
    "    'starts_with_cap',\n",
    "    'has_url',\n",
    "    'has_number',\n",
    "    'has_mention',\n",
    "    'has_hashtag',\n",
    "    'has_email'\n",
    "]\n",
    "\n",
    "df_corr = df_features[df_features['is_toxic']!=np.nan]\n",
    "correlations = df_corr[composed_features + ['is_toxic']].corr()['is_toxic'].drop('is_toxic')\n",
    "\n",
    "# Plot tops of the correlated: \n",
    "correlations_sorted = correlations.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=correlations_sorted.values, y=correlations_sorted.index, palette=\"viridis\")\n",
    "plt.title('Correlation of composed features with is_toxic')\n",
    "plt.xlabel('Correlation with is_toxic')\n",
    "plt.ylabel('Features')\n",
    "plt.xlim(-0.3, 0.3)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "* `count_profanity`, `starts_with_cap`, `is_all_lower`, `has_punctuation_spp`, `has_mention`, `has_number` features have quite high correlations with `is_toxic`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Types-of-feature correlations\n",
    "ALso, the separate tables will be created to define the next types of the correlations: \n",
    "1. [`raw_text_id` - `Types_of_emoji_used`].csv vs `target`\n",
    "2. [`raw_text_id` - `Types_of_emoticon_used`].csv vs `target`\n",
    "3. [`raw_text_id` - `Types_of_repeating_punctuation_used`].csv vs `target`\n",
    "4. [`raw_text_id` - `Types_of_separate_punctuation_used`].csv vs `target`\n",
    "\n",
    "If some strong correlation will be found for the specific types of the emojis/punct etc.,  \n",
    "we can use it as the additional text features during the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set up composed jsons: \n",
    "json_files = {\n",
    "    'profanities': 'encoding_profanities.json',\n",
    "    'rep_punct': 'encoding_rep_punct.json',\n",
    "    'sep_punct': 'encoding_sep_punct.json',\n",
    "    'emoji': 'encoding_emoji.json',\n",
    "    'emoticon': 'encoding_emoticon.json'\n",
    "}\n",
    "\n",
    "# Read dictionaries: \n",
    "encodings = {}\n",
    "for key, file in json_files.items():\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        encodings[key] = json.load(f)\n",
    "\n",
    "# Compose correlation dataframes for the each vaule of different tokens categories:\n",
    "df_category = {}\n",
    "for category, token_dict in encodings.items():\n",
    "    cols = list(token_dict.keys())\n",
    "    \n",
    "    # merging by the raw_text_id: \n",
    "    df_temp = pd.DataFrame({'raw_text_id': df_features['raw_text_id']}) \n",
    "    for token in cols:\n",
    "        df_temp[token] = df_features['text_encoded_profanity'].str.count(re.escape(token))\n",
    "    df_category[category] = df_temp\n",
    "    df_temp.to_csv(f'df_{category}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_correlations(features_csv, original_csv, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate pairwise correlations between is_toxic and the additional created features \n",
    "    original_csv must have raw_text_id, is_toxic columns for merging \n",
    "    \"\"\"\n",
    "    \n",
    "    df_features_new = pd.read_csv(features_csv)\n",
    "    df_original = pd.read_csv(original_csv)\n",
    "    \n",
    "    # merge on raw_text_id: \n",
    "    df_merged = df_features_new.merge(df_original[['raw_text_id', 'is_toxic']], on='raw_text_id', how='left')\n",
    "    df_merged = df_merged[df_merged['is_toxic'].notna()]\n",
    "    \n",
    "    # calculate correlations with is_toxic\n",
    "    token_cols = df_features_new.columns.drop('raw_text_id')\n",
    "    correlations = df_merged[token_cols].corrwith(df_merged['is_toxic'])\n",
    "    \n",
    "    # get top-N correlations by absolute value\n",
    "    top_corr = correlations.reindex(correlations.abs().sort_values(ascending=False).index).head(top_n)\n",
    "    \n",
    "    return top_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "for category in ['emoticon', 'emoji', 'rep_punct', 'sep_punct']:\n",
    "        \n",
    "    top_corr = compute_top_correlations(f'df_{category}.csv', 'dump_features_0.csv', top_n=10)\n",
    "    print(f\"\\nTop correlations for {category}:\")\n",
    "    print(top_corr)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## 3. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "There're a few features could be added (or considered to be added) to the inputs of classical ML   \n",
    "(it can be concatenated with TF-IDF numerical features, for example): \n",
    "* `count_profanity`, `starts_with_cap`, `is_all_lower`, `has_punctuation_spp`, `has_mention`, `has_number` \n",
    "* the rest from `composed_features` are additional and can be considered experimentally   \n",
    "<!-- * `[URL]`, `[NUM]`, `[MNT]`, `[HSG]`,`[EML]`, substrings can be deleted from text -->\n",
    "\n",
    "The negative conclusions are: \n",
    "* There're not enough count of emojis in the dataset to calculate correlations correctly (or they're not so frequent/informative)\n",
    "* The pairwise correlations for types of `emoji_n`/`punctuation_n`/`profanity_n` are less than their common interpretations (`has_emoji`, `count_profanity`, `count_spp` etc.), so there's no need to include them into inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
