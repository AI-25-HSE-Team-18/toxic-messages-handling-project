{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# First classic ML models training\n",
    "---\n",
    "\n",
    "## Notebook contents ##  \n",
    "* Simple linear models training\n",
    "* Selection of the best model, method\n",
    "* Simple improvements of model, GridSearch for example\n",
    "* Conclusions\n",
    "\n",
    "`NOTE`: average `macro f1_score` selected as the target metric, anothers can be used as additional.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast \n",
    "import re\n",
    "import warnings \n",
    "# import pymorphy2\n",
    "\n",
    "# import kagglehub\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Data taken from `text_domain_features_0.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and read data: \n",
    "df_path = 'dump_features_include_numeric.csv'\n",
    "df = pd.read_csv(df_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now we're taking dataset_id 0, 1 to train, and the another ones (2-7) to test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data from the cleanest datasets (0, 1) to train: \n",
    "df_train = df[(df['dataset_id']==0) | (df['dataset_id']==1)]\n",
    "df_train.dropna(subset='is_toxic', inplace=True)\n",
    "\n",
    "# select data from the same source (vk), with the best results that BERT classifier predicted: \n",
    "df_test = df[df['dataset_id']==7]\n",
    "\n",
    "n_test_add = 60000 \n",
    "df_test = pd.concat(\n",
    "    [\n",
    "    df_test, \n",
    "    df[(df['dataset_id']==2) | (df['dataset_id']==3) | (df['dataset_id']==4) | (df['dataset_id']==5)].dropna(subset='is_toxic').sample(\n",
    "        n=n_test_add,\n",
    "        random_state=random_state)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "print(f'df_train shape: {df_train.shape}')\n",
    "print(f'df_test shape: {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count train is_toxic distribution:\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='is_toxic', data=df_train.dropna(subset='is_toxic'), palette=['mediumseagreen','salmon'])\n",
    "plt.title('Distribution of is_toxic variable in the chosen train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count test is_toxic distribution:\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='is_toxic', data=df_test, palette=['mediumseagreen','salmon'])\n",
    "plt.title('Distribution of is_toxic variable in the chosen test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_models(\n",
    "        train_texts: pd.Series,\n",
    "        train_targets: pd.Series,\n",
    "        train_num_features: pd.DataFrame,\n",
    "\n",
    "        test_texts: pd.Series,\n",
    "        test_targets: pd.Series,\n",
    "        test_num_features: pd.DataFrame,\n",
    "\n",
    "        # for results params:\n",
    "        use_num_features: bool = True,\n",
    "        texts_col: str = '',\n",
    "        downsample: bool = False\n",
    "    ):\n",
    "    \n",
    "    \"\"\"Train basic models. Tha main goal is to to find the best one model.  \n",
    "\n",
    "    train_texts, test_texts - pd.Series columns from base dataset that uses preprocessed texts.  \n",
    "    train_targets, test_targets - pd.Series columns from base dataset that   \n",
    "    train_num_features, test_num_features include additional pd.Dataframe that was composed in text domain  \n",
    "    use_num_features uses bool var to define wwhether to add numerical features into experiment or not  \n",
    "    \"\"\"\n",
    "\n",
    "    # apply tf-idf: \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.9\n",
    "    )\n",
    "    X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "    X_test_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "    if use_num_features:\n",
    "        # check whether all columns in train, test are appropriating to each other,\n",
    "        # correct if not:  \n",
    "        train_num_features, test_num_features = train_num_features.align(\n",
    "            test_num_features, axis=1, join=\"left\", fill_value=0\n",
    "        )\n",
    "\n",
    "        # tf-idf is sparse, so compose numeric features into sparse format too: \n",
    "        train_num_sparse = csr_matrix(train_num_features.values)\n",
    "        test_num_sparse = csr_matrix(test_num_features.values)\n",
    "\n",
    "        # concat vectorized text and numeric features: \n",
    "        X_train = hstack([X_train_tfidf, train_num_sparse])\n",
    "        X_test = hstack([X_test_tfidf, test_num_sparse])\n",
    "    else:\n",
    "        X_train = X_train_tfidf\n",
    "        X_test = X_test_tfidf\n",
    "\n",
    "    \n",
    "    # select basic model examples using standart params only\n",
    "    # the best model will be selected and tried to improve in its hypermarams: \n",
    "    # models = {\n",
    "    #     \"Logistic Regression\": LogisticRegression(class_weight=\"balanced\"),\n",
    "    #     \"Linear SVM (LinearSVC)\": LinearSVC(class_weight=\"balanced\"),\n",
    "    #     \"SGD Classifier\": SGDClassifier(class_weight=\"balanced\")\n",
    "    # }\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Linear SVM (LinearSVC)\": LinearSVC(),\n",
    "        \"SGD Classifier\": SGDClassifier()\n",
    "    }\n",
    "\n",
    "    # metric results: \n",
    "    results = []\n",
    "    \n",
    "    # train and eval: \n",
    "    for name, model in models.items():\n",
    "        # print(\"=\" * 60)\n",
    "        # print(f\"Using model:: {name}\")\n",
    "        model.fit(X_train, train_targets)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        macro_f1 = f1_score(test_targets, preds, average=\"macro\")\n",
    "        micro_f1 = f1_score(test_targets, preds, average=\"micro\")\n",
    "\n",
    "        # base target metrics and additional output to eval metrics complexely: \n",
    "        # print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "        # print(f\"Micro F1-score: {micro_f1:.4f}\")\n",
    "\n",
    "        # compose cls report and take the metrics from it: \n",
    "        report = classification_report(test_targets, preds, output_dict=True)\n",
    "        class_metrics = {}\n",
    "        for cls in sorted(report.keys()):\n",
    "            if cls.replace('.', '', 1).isdigit(): \n",
    "                cls_text = int(float(cls))\n",
    "                class_metrics[f\"class_{cls_text}_precision\"] = report[cls][\"precision\"]\n",
    "                class_metrics[f\"class_{cls_text}_recall\"] = report[cls][\"recall\"]\n",
    "\n",
    "        # update the record by values and add to the list: \n",
    "        record = {\n",
    "            \"model\": name,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"micro_f1\": micro_f1,\n",
    "            \"use_num_features\": use_num_features, \n",
    "            \"texts_col\": texts_col,\n",
    "            \"downsampled_to_equals\": downsample           \n",
    "        }\n",
    "        record.update(class_metrics)\n",
    "        results.append(record)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init results to compose the metrics: \n",
    "results = [] \n",
    "\n",
    "target_col = 'is_toxic'\n",
    "composed_features = [\n",
    "    'count_spp', 'count_rpp', 'punct_after_space', 'has_emoji', 'has_emoticon',\n",
    "    'has_capslock', 'is_all_lower', 'has_punctuation_spp', 'has_punctuation_rpp',\n",
    "    'has_fence_ironic_style', 'count_profanity', 'has_pronouns', 'starts_with_cap',\n",
    "    'has_url', 'has_number', 'has_mention', 'has_hashtag', 'ends_with_dot',\n",
    "    'has_emotional_sym', 'has_repeating_letters_3plus'\n",
    "]\n",
    "\n",
    "\n",
    "for downsample in [False, True]: \n",
    "    if downsample: \n",
    "        # ss a temporary solution, \n",
    "        # select the n_1 random values from the train set that equal to 0 in is_toxic: \n",
    "        # n_1 = df_train[df_train['is_toxic']==1].shape[0]\n",
    "        # n_0 = df_train[df_train['is_toxic']==0].shape[0]\n",
    "\n",
    "        # downsampling of train: \n",
    "        n_1 = df_train[df_train['is_toxic'] == 1].shape[0]\n",
    "        df_0 = df_train[df_train['is_toxic'] == 0]\n",
    "        df_1 = df_train[df_train['is_toxic'] == 1]\n",
    "        df_0_down = df_0.sample(n=n_1, random_state=42)\n",
    "        df_train_copy = pd.concat([df_0_down, df_1], axis=0)\n",
    "\n",
    "        # downsampling of test:\n",
    "        n_1 = df_test[df_test['is_toxic'] == 1].shape[0]\n",
    "        df_0 = df_test[df_test['is_toxic'] == 0]\n",
    "        df_1 = df_test[df_test['is_toxic'] == 1]\n",
    "        df_0_down = df_0.sample(n=n_1, random_state=42)\n",
    "        df_test_copy = pd.concat([df_0_down, df_1], axis=0)\n",
    "    else: \n",
    "        df_train_copy = df_train.copy()\n",
    "        df_test_copy = df_test.copy()\n",
    "        \n",
    "    for use_num in [True, False]: # select whether to choose numerical features or not\n",
    "\n",
    "        for texts_col in ['text_raw', 'text_encoded_profanity', 'text_del_stop_words', 'text_without_tokens']:\n",
    "            \n",
    "            # prepare data: \n",
    "            df_train_clean = df_train_copy.dropna(subset=composed_features + [texts_col]).copy()\n",
    "            df_test_clean = df_test_copy.dropna(subset=composed_features + [texts_col]).copy()\n",
    "\n",
    "            # select appropriate rows: \n",
    "            train_texts = df_train_clean[texts_col]\n",
    "            train_targets = df_train_clean[target_col]\n",
    "            train_num_features = df_train_clean[composed_features]\n",
    "\n",
    "            test_texts = df_test_clean[texts_col]\n",
    "            test_targets = df_test_clean[target_col]\n",
    "            test_num_features = df_test_clean[composed_features]\n",
    "\n",
    "            # run models: \n",
    "            model_reports = train_and_test_models(\n",
    "                train_texts,\n",
    "                train_targets,\n",
    "                train_num_features,\n",
    "                test_texts,\n",
    "                test_targets,\n",
    "                test_num_features,\n",
    "                \n",
    "                # params to register results: \n",
    "                use_num_features=use_num,\n",
    "                texts_col=texts_col,\n",
    "                downsample=downsample\n",
    "            )\n",
    "            results.extend(model_reports)\n",
    "\n",
    "    # convert to df: \n",
    "    results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Let's take a look on the results and find the best model and method at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top n: \n",
    "top_n = 8\n",
    "results_df.sort_values(\n",
    "    # by=['macro_f1', 'micro_f1', 'class_0_precision', 'class_0_recall', 'class_1_precision', 'class_1_recall'], \n",
    "    by=['macro_f1'], \n",
    "    ascending=False, inplace=True)\n",
    "results_df.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(results_df.phik_matrix())\n",
    "plt.suptitle('The impact of experimental approaches on metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Short conclusions are: \n",
    "* The methods used, `use_num_features` (which means adding the numerical features composed in text domain empirically), and `downsampled_to_equals` (which means downsampling ==0 values to balance the set), affect precision and recall more strongly than they affect the macro_f1 score.\n",
    "* `downsampled_to_equals` and `use_num_features` affect recall more than precision\n",
    "* The most dangerous metric is recall\n",
    "\n",
    "The best model: \n",
    "* Is Linear SVM (LinearSVC)\n",
    "* Is adding the numerical features to the train+test set\n",
    "* Is not using downsampling for train, test\n",
    "\n",
    "and it has metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
